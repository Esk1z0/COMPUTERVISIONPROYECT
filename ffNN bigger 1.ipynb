{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "QYuALZOG-AMq",
    "tags": []
   },
   "source": [
    "## Assignment: Image recognition\n",
    "- Alumno 1:\n",
    "- Alumno 2:\n",
    "- Alumno 3:\n",
    "\n",
    "The goals of the assignment are:\n",
    "* Develop proficiency in using Tensorflow/Keras for training Neural Nets (NNs).\n",
    "* Put into practice the acquired knowledge to optimize the parameters and architecture of a feedforward Neural Net (ffNN), in the context of an image recognition problem.\n",
    "* Put into practice NNs specially conceived for analysing images. Design and optimize the parameters of a Convolutional Neural Net (CNN) to deal with previous task.\n",
    "* Train popular architectures from scratch (e.g., GoogLeNet, VGG, ResNet, ...), and compare the results with the ones provided by their pre-trained versions using transfer learning.\n",
    "\n",
    "Follow the link below to download the classification data set  “xview_recognition”: [https://drive.upm.es/s/2DDPE2zHw5dbM3G](https://drive.upm.es/s/2DDPE2zHw5dbM3G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:21.031186Z",
     "start_time": "2024-10-26T00:00:17.131476Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-09-18T08:27:59.622636Z",
     "iopub.status.busy": "2025-09-18T08:27:59.622372Z",
     "iopub.status.idle": "2025-09-18T08:28:13.071424Z",
     "shell.execute_reply": "2025-09-18T08:28:13.070171Z"
    },
    "executionInfo": {
     "elapsed": 9912,
     "status": "ok",
     "timestamp": 1759687663156,
     "user": {
      "displayName": "Jimmy “Jimmy”",
      "userId": "16087473281499178535"
     },
     "user_tz": -120
    },
    "id": "i7H8WLjBzB2n",
    "outputId": "d5027c76-e627-45d5-ff51-2f270bbe7d2f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME=\"Bigger\"\n",
    "assert(not EXPERIMENT_NAME is None)\n",
    "\n",
    "import tensorflow as tf\n",
    "import uuid\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import rasterio\n",
    "import json\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.callbacks import TerminateOnNaN, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_PATH = \"/workspace/COMPUTERVISIONPROYECT\"\n",
    "DATASET_NAME = 'stratified'\n",
    "\n",
    "# XVIEW_RECOGNITION_PATH = \"./data/xview_recognition/\"\n",
    "XVIEW_RECOGNITION_PATH = os.path.join(PROJECT_PATH, \"data/xview_recognition\")\n",
    "\n",
    "EXPERIMENT_PATH = os.path.join(PROJECT_PATH, \"experiment_results\", EXPERIMENT_NAME)\n",
    "assert(not os.path.exists(EXPERIMENT_PATH))\n",
    "if not os.path.exists(EXPERIMENT_PATH):\n",
    "    os.mkdir(EXPERIMENT_PATH)\n",
    "BEST_WEIGHTS_FILENAME = os.path.join(EXPERIMENT_PATH, 'best_weights.keras')\n",
    "MODEL_PATH = EXPERIMENT_PATH\n",
    "\n",
    "TRAIN_SET_FILENAME = '_'.join([DATASET_NAME, 'train.tfrecord'])\n",
    "VALIDATION_SET_FILENAME = '_'.join([DATASET_NAME, 'validation.tfrecord'])\n",
    "TRAIN_SET_PATH = os.path.join(XVIEW_RECOGNITION_PATH, TRAIN_SET_FILENAME)\n",
    "VALIDATION_SET_PATH = os.path.join(XVIEW_RECOGNITION_PATH, VALIDATION_SET_FILENAME)\n",
    "\n",
    "## Hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = None\n",
    "DROPOUT_RATE = None\n",
    "REGULARIZER_PENALTY = None\n",
    "EPOCHS = 20\n",
    "\n",
    "categories = {0: 'Cargo plane', 1: 'Small car', 2: 'Bus', 3: 'Truck', 4: 'Motorboat', 5: 'Fishing vessel', 6: 'Dump truck', 7: 'Excavator', 8: 'Building', 9: 'Helipad', 10: 'Storage tank', 11: 'Shipping container', 12: 'Pylon'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:21.066937Z",
     "start_time": "2024-10-26T00:00:21.059126Z"
    },
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-09-18T08:28:13.076611Z",
     "iopub.status.busy": "2025-09-18T08:28:13.076007Z",
     "iopub.status.idle": "2025-09-18T08:28:13.083344Z",
     "shell.execute_reply": "2025-09-18T08:28:13.082866Z"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1759687663176,
     "user": {
      "displayName": "Jimmy “Jimmy”",
      "userId": "16087473281499178535"
     },
     "user_tz": -120
    },
    "id": "OYtqD3Oh-AMw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GenericObject:\n",
    "    \"\"\"\n",
    "    Generic object data.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.id = uuid.uuid4()\n",
    "        self.bb = (-1, -1, -1, -1)\n",
    "        self.category= -1\n",
    "        self.score = -1\n",
    "\n",
    "class GenericImage:\n",
    "    \"\"\"\n",
    "    Generic image data.\n",
    "    \"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.tile = np.array([-1, -1, -1, -1])  # (pt_x, pt_y, pt_x+width, pt_y+height)\n",
    "        self.objects = list([])\n",
    "\n",
    "    def add_object(self, obj: GenericObject):\n",
    "        self.objects.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confusion_matrix(cm, categories):\n",
    "    # Draw confusion matrix\n",
    "    fig = plt.figure(figsize=[3.2*pow(len(categories), 0.5), 2.4*pow(len(categories), 0.5)])\n",
    "    ax = fig.add_subplot(111)\n",
    "    cm = cm.astype('float') / np.maximum(cm.sum(axis=1)[:, np.newaxis], np.finfo(np.float64).eps)\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.colormaps['Blues'])\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), xticklabels=list(categories.values()), yticklabels=list(categories.values()), ylabel='Annotation', xlabel='Prediction')\n",
    "    # Rotate the tick labels and set their alignment\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    # Loop over data dimensions and create text annotations\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], '.2f'), ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=int(15-pow(len(categories), 0.5)))\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 0. La usamos para las imágenes de test\n",
    "def load_geoimage(filename_tensor):\n",
    "    warnings.filterwarnings('ignore', category=rasterio.errors.NotGeoreferencedWarning)\n",
    "    filename = filename_tensor\n",
    "    src_raster = rasterio.open(filename, 'r')\n",
    "\n",
    "    input_type = src_raster.profile['dtype']\n",
    "    input_channels = src_raster.count\n",
    "    img = np.zeros((src_raster.height, src_raster.width, src_raster.count), dtype=input_type)\n",
    "    for band in range(input_channels):\n",
    "        img[:, :, band] = src_raster.read(band+1)\n",
    "    return img\n",
    "\n",
    "# 1. Función para decodificar los ejemplos del archivo\n",
    "def _parse_image_function(example_proto):\n",
    "    feature_description = {\n",
    "        'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'category_id': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    \n",
    "    features = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    \n",
    "    image = tf.io.decode_raw(features['image_raw'], out_type=tf.uint8)\n",
    "    shape = [features['height'], features['width'], features['depth']]\n",
    "    image = tf.reshape(image, shape)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    \n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    \n",
    "    category_id = tf.cast(features['category_id'], tf.int32)\n",
    "    label = tf.one_hot(category_id, depth=len(categories))\n",
    "    \n",
    "    return image, label\n",
    "\n",
    "# 2. Función para crear el dataset\n",
    "def create_tfrecord_dataset(filenames, batch_size, do_shuffle=False):\n",
    "    # Lee los archivos TFRecord. Puede leer de múltiples archivos en paralelo.\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=tf.data.AUTOTUNE)\n",
    "    \n",
    "    dataset = dataset.map(_parse_image_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.cache()\n",
    "    \n",
    "    if do_shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratification comparison and weighing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Train vs Validation distribution drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_tfrecord_dataset([os.path.join(XVIEW_RECOGNITION_PATH, 'train.tfrecord')], BATCH_SIZE, do_shuffle=True)\n",
    "valid_dataset = create_tfrecord_dataset([os.path.join(XVIEW_RECOGNITION_PATH, 'validation.tfrecord')], BATCH_SIZE)\n",
    "\n",
    "train_iter = iter(train_dataset)\n",
    "\n",
    "y_train_labels = []\n",
    "for _,label_idxs in train_iter:\n",
    "    y_train_labels += list(label_idxs.numpy().argmax(axis=1))\n",
    "    \n",
    "validation_iter = iter(valid_dataset)\n",
    "\n",
    "y_validation_labels = []\n",
    "for _,label_idxs in validation_iter:\n",
    "    y_validation_labels += list(label_idxs.numpy().argmax(axis=1))\n",
    "\n",
    "train_counts = pd.Series(y_train_labels).value_counts()\n",
    "validation_counts = pd.Series(y_validation_labels).value_counts()\n",
    "\n",
    "total_train = len(y_train_labels)\n",
    "total_validation = len(y_validation_labels)\n",
    "\n",
    "sorted_class_ids = train_counts.index\n",
    "\n",
    "train_percentages = (train_counts / total_train) * 100\n",
    "validation_percentages = (validation_counts / total_validation) * 100\n",
    "\n",
    "sorted_class_ids = train_counts.index\n",
    "validation_percentages = validation_percentages.reindex(sorted_class_ids, fill_value=0)\n",
    "\n",
    "category_labels = [categories[cid] for cid in sorted_class_ids]\n",
    "\n",
    "x = np.arange(len(category_labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "rects1 = ax.bar(x - width/2, train_percentages, width, label='Train Set', color='cornflowerblue')\n",
    "rects2 = ax.bar(x + width/2, validation_percentages, width, label='Validation Set', color='sandybrown')\n",
    "\n",
    "ax.set_ylabel('Número de Muestras', fontsize=12)\n",
    "ax.set_title('Distribución de Clases en los Sets de Train y Validation', fontsize=14, weight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(category_labels, rotation=40, ha=\"right\")\n",
    "ax.legend()\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "ax.bar_label(rects1, padding=3, fmt='%.1f%%')\n",
    "ax.bar_label(rects2, padding=3, fmt='%.1f%%')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No distribution drift after stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_tfrecord_dataset([os.path.join(XVIEW_RECOGNITION_PATH, 'stratified_train.tfrecord')], BATCH_SIZE, do_shuffle=True)\n",
    "valid_dataset = create_tfrecord_dataset([os.path.join(XVIEW_RECOGNITION_PATH, 'stratified_validation.tfrecord')], BATCH_SIZE)\n",
    "\n",
    "train_iter = iter(train_dataset)\n",
    "\n",
    "y_train_labels = []\n",
    "for _,label_idxs in train_iter:\n",
    "    y_train_labels += list(label_idxs.numpy().argmax(axis=1))\n",
    "    \n",
    "validation_iter = iter(valid_dataset)\n",
    "\n",
    "y_validation_labels = []\n",
    "for _,label_idxs in validation_iter:\n",
    "    y_validation_labels += list(label_idxs.numpy().argmax(axis=1))\n",
    "\n",
    "train_counts = pd.Series(y_train_labels).value_counts()\n",
    "validation_counts = pd.Series(y_validation_labels).value_counts()\n",
    "\n",
    "total_train = len(y_train_labels)\n",
    "total_validation = len(y_validation_labels)\n",
    "\n",
    "sorted_class_ids = train_counts.index\n",
    "\n",
    "train_percentages = (train_counts / total_train) * 100\n",
    "validation_percentages = (validation_counts / total_validation) * 100\n",
    "\n",
    "sorted_class_ids = train_counts.index\n",
    "validation_percentages = validation_percentages.reindex(sorted_class_ids, fill_value=0)\n",
    "\n",
    "category_labels = [categories[cid] for cid in sorted_class_ids]\n",
    "\n",
    "x = np.arange(len(category_labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "rects1 = ax.bar(x - width/2, train_percentages, width, label='Train Set', color='cornflowerblue')\n",
    "rects2 = ax.bar(x + width/2, validation_percentages, width, label='Validation Set', color='sandybrown')\n",
    "\n",
    "ax.set_ylabel('Número de Muestras', fontsize=12)\n",
    "ax.set_title('Distribución de Clases en los Sets de Train y Validation', fontsize=14, weight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(category_labels, rotation=40, ha=\"right\")\n",
    "ax.legend()\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "ax.bar_label(rects1, padding=3, fmt='%.1f%%')\n",
    "ax.bar_label(rects2, padding=3, fmt='%.1f%%')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(class_weight='balanced',\n",
    "                                     classes=np.unique(y_train_labels),\n",
    "                                     y=y_train_labels)\n",
    "w = pd.Series(class_weights, index = [categories[x] for x in np.unique(y_train_labels)], name='Weight Classes')\n",
    "w.index.name='Category'\n",
    "classes_weights = w.sort_values(ascending=False).to_frame().reset_index()\n",
    "classes_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diNBB3qy-AM2"
   },
   "source": [
    "# Training\n",
    "Design and train a ffNN to deal with the “xview_recognition” classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = pd.Series(class_weights, index = [categories[x] for x in np.unique(y_train_labels)], name='Weight Classes')\n",
    "w.index.name='Category'\n",
    "w.sort_values(ascending=False).to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_tfrecord_dataset([TRAIN_SET_PATH], BATCH_SIZE, do_shuffle=True)\n",
    "valid_dataset = create_tfrecord_dataset([VALIDATION_SET_PATH], BATCH_SIZE)\n",
    "\n",
    "train_iter = iter(train_dataset)\n",
    "\n",
    "y_train_labels = []\n",
    "for _,label_idxs in train_iter:\n",
    "    y_train_labels += list(label_idxs.numpy().argmax(axis=1))\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced',\n",
    "                                     classes=np.unique(y_train_labels),\n",
    "                                     y=y_train_labels)\n",
    "\n",
    "index,freqs = np.unique(y_train_labels, return_counts=True)\n",
    "categories_count = pd.Series(freqs, index=[categories[idx] for idx in index], name='Size').sort_values(ascending=False)\n",
    "\n",
    "categories_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 761
    },
    "executionInfo": {
     "elapsed": 1848,
     "status": "ok",
     "timestamp": 1759687665824,
     "user": {
      "displayName": "Jimmy “Jimmy”",
      "userId": "16087473281499178535"
     },
     "user_tz": -120
    },
    "id": "Hby939K1WvZy",
    "outputId": "0d07705a-3ccd-4e4b-af42-debc9c17fdc6"
   },
   "outputs": [],
   "source": [
    "print('Load model')\n",
    "\n",
    "model = Sequential(name=EXPERIMENT_NAME)\n",
    "model.add(Flatten(input_shape=(224, 224, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(len(categories)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:25.467525Z",
     "start_time": "2024-10-26T00:00:25.434068Z"
    },
    "execution": {
     "iopub.execute_input": "2025-09-18T08:28:16.337697Z",
     "iopub.status.busy": "2025-09-18T08:28:16.337546Z",
     "iopub.status.idle": "2025-09-18T08:28:16.351042Z",
     "shell.execute_reply": "2025-09-18T08:28:16.350124Z"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1759687665849,
     "user": {
      "displayName": "Jimmy “Jimmy”",
      "userId": "16087473281499178535"
     },
     "user_tz": -120
    },
    "id": "-aSlKtG6-AM7"
   },
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T00:00:26.254555Z",
     "start_time": "2024-10-26T00:00:26.243908Z"
    },
    "execution": {
     "iopub.execute_input": "2025-09-18T08:28:16.352904Z",
     "iopub.status.busy": "2025-09-18T08:28:16.352762Z",
     "iopub.status.idle": "2025-09-18T08:28:16.356742Z",
     "shell.execute_reply": "2025-09-18T08:28:16.356290Z"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1759687665863,
     "user": {
      "displayName": "Jimmy “Jimmy”",
      "userId": "16087473281499178535"
     },
     "user_tz": -120
    },
    "id": "GGAJEfpB-AM8"
   },
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "model_checkpoint = ModelCheckpoint(BEST_WEIGHTS_FILENAME, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.5, patience=5, verbose=1, min_lr=1e-6)\n",
    "early_stop = EarlyStopping('val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "terminate = TerminateOnNaN()\n",
    "callbacks = [model_checkpoint, reduce_lr, early_stop, terminate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 304741,
     "status": "ok",
     "timestamp": 1759687970646,
     "user": {
      "displayName": "Jimmy “Jimmy”",
      "userId": "16087473281499178535"
     },
     "user_tz": -120
    },
    "id": "IWUfJ9fiYa6H",
    "outputId": "89cbc010-88e3-4fc2-e8c3-d06d1c6b4b94"
   },
   "outputs": [],
   "source": [
    "print('Training model')\n",
    "h = model.fit(train_dataset, \n",
    "              validation_data=valid_dataset, \n",
    "              epochs=EPOCHS, \n",
    "              callbacks=callbacks, \n",
    "              verbose=1,\n",
    "              class_weight=dict(enumerate(class_weights)))\n",
    "\n",
    "# Best validation model (using validation loss)\n",
    "best_idx = int(np.argmin(h.history['val_loss']))\n",
    "best_values = h.history['val_accuracy'][best_idx], h.history['val_loss'][best_idx]\n",
    "print(\n",
    "    f'''--------\\n\n",
    "    Best validation model\n",
    "    - epoch: {str(best_idx + 1)}\n",
    "    - val_acc: {str(best_values[0])}\n",
    "    - val_loss: {str(best_values[1])}'''\n",
    ")\n",
    "model.load_weights(BEST_WEIGHTS_FILENAME)\n",
    "model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "8IMMO_mT-AM9",
    "tags": []
   },
   "source": [
    "# Validation\n",
    "Compute validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(MODEL_PATH)\n",
    "valid_dataset = create_tfrecord_dataset([os.path.join(XVIEW_RECOGNITION_PATH, 'validation.tfrecord')], BATCH_SIZE)\n",
    "\n",
    "predictions = model.predict(valid_dataset)\n",
    "category_names = np.array(list(categories.values()))\n",
    "y_pred = category_names[predictions.argmax(axis=1)]\n",
    "\n",
    "y_true = []\n",
    "\n",
    "for images_batch, labels_batch in valid_dataset:\n",
    "    labels_np = labels_batch.numpy()\n",
    "    \n",
    "    true_indices = np.argmax(labels_np, axis=1)\n",
    "    batch_labels = category_names[true_indices]\n",
    "    \n",
    "    y_true.extend(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 954
    },
    "execution": {
     "iopub.execute_input": "2025-09-18T13:01:22.019704Z",
     "iopub.status.busy": "2025-09-18T13:01:22.019551Z",
     "iopub.status.idle": "2025-09-18T13:01:22.740150Z",
     "shell.execute_reply": "2025-09-18T13:01:22.739387Z"
    },
    "executionInfo": {
     "elapsed": 1278,
     "status": "ok",
     "timestamp": 1759688142465,
     "user": {
      "displayName": "Jimmy “Jimmy”",
      "userId": "16087473281499178535"
     },
     "user_tz": -120
    },
    "id": "F_FmeGytzB2y",
    "outputId": "84f8d5e1-a52e-432a-f836-f9d10a237ca1"
   },
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(categories.values()))\n",
    "draw_confusion_matrix(cm, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-18T13:01:22.742591Z",
     "iopub.status.busy": "2025-09-18T13:01:22.742443Z",
     "iopub.status.idle": "2025-09-18T13:01:22.750853Z",
     "shell.execute_reply": "2025-09-18T13:01:22.749931Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1759688142471,
     "user": {
      "displayName": "Jimmy “Jimmy”",
      "userId": "16087473281499178535"
     },
     "user_tz": -120
    },
    "id": "I1Ash4SUzB2y",
    "outputId": "10ff3d16-b18c-4db4-c2aa-321a7b33e0f6"
   },
   "outputs": [],
   "source": [
    "ll = []\n",
    "# Compute the accuracy\n",
    "correct_samples_class = np.diag(cm).astype(float)\n",
    "total_samples_class = np.sum(cm, axis=1).astype(float)\n",
    "total_predicts_class = np.sum(cm, axis=0).astype(float)\n",
    "print('Mean Accuracy: %.3f%%' % (np.sum(correct_samples_class) / np.sum(total_samples_class) * 100))\n",
    "acc = correct_samples_class / np.maximum(total_samples_class, np.finfo(np.float64).eps)\n",
    "print('Mean Recall: %.3f%%' % (acc.mean() * 100))\n",
    "acc = correct_samples_class / np.maximum(total_predicts_class, np.finfo(np.float64).eps)\n",
    "print('Mean Precision: %.3f%%' % (acc.mean() * 100))\n",
    "for idx in range(len(categories)):\n",
    "    # True/False Positives (TP/FP) refer to the number of predicted positives that were correct/incorrect.\n",
    "    # True/False Negatives (TN/FN) refer to the number of predicted negatives that were correct/incorrect.\n",
    "    tp = cm[idx, idx]\n",
    "    fp = sum(cm[:, idx]) - tp\n",
    "    fn = sum(cm[idx, :]) - tp\n",
    "    tn = sum(np.delete(sum(cm) - cm[idx, :], idx))\n",
    "    # True Positive Rate: proportion of real positive cases that were correctly predicted as positive.\n",
    "    recall = tp / np.maximum(tp+fn, np.finfo(np.float64).eps)\n",
    "    # Precision: proportion of predicted positive cases that were truly real positives.\n",
    "    precision = tp / np.maximum(tp+fp, np.finfo(np.float64).eps)\n",
    "    # True Negative Rate: proportion of real negative cases that were correctly predicted as negative.\n",
    "    specificity = tn / np.maximum(tn+fp, np.finfo(np.float64).eps)\n",
    "    # Dice coefficient refers to two times the intersection of two sets divided by the sum of their areas.\n",
    "    # Dice = 2 |A∩B| / (|A|+|B|) = 2 TP / (2 TP + FP + FN)\n",
    "    f1_score = 2 * ((precision * recall) / np.maximum(precision+recall, np.finfo(np.float64).eps))\n",
    "    ll.append(pd.Series([recall, precision, specificity, f1_score], name=categories[idx], index=[\"Recall\", \"Precision\", \"Specificity\", \"F1 Score\"]))\n",
    "ll = pd.DataFrame(ll)\n",
    "ll = pd.concat([categories_count, ll, pd.Series([EXPERIMENT_NAME for _ in categories], name='Experiment', index=categories.values())],axis=1)\n",
    "ll.index.name='Category'\n",
    "ll = ll.reset_index().set_index(['Experiment','Category'])\n",
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "plt.title('Accuracy & Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "\n",
    "color_acc = 'blue'\n",
    "ax1.set_ylabel('Accuracy', color=color_acc)\n",
    "ax1.plot(h.history['accuracy'], label='Train Accuracy', color='blue', marker='.')\n",
    "ax1.plot(h.history['val_accuracy'], label='Validation Accuracy', color='tab:blue', linestyle='--', marker='.')\n",
    "ax1.tick_params(axis='y', labelcolor=color_acc)\n",
    "ax1.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "ax2 = ax1.twinx()  \n",
    "color_loss = 'orange'\n",
    "ax2.set_ylabel('Loss', color=color_loss)\n",
    "ax2.plot(h.history['loss'], label='Train Loss', color='orange', marker='.')\n",
    "ax2.plot(h.history['val_loss'], label='Validation Loss', color='tab:orange', linestyle='--', marker='.')\n",
    "ax2.tick_params(axis='y', labelcolor=color_loss)\n",
    "\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='center left', frameon=True, fontsize='small')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0ppnBZszB2z"
   },
   "source": [
    "# Testing\n",
    "Try to improve the results provided in the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-18T13:01:22.753680Z",
     "iopub.status.busy": "2025-09-18T13:01:22.753432Z",
     "iopub.status.idle": "2025-09-18T13:01:22.832351Z",
     "shell.execute_reply": "2025-09-18T13:01:22.831293Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1759688142476,
     "user": {
      "displayName": "Jimmy “Jimmy”",
      "userId": "16087473281499178535"
     },
     "user_tz": -120
    },
    "id": "tJr_-xCt-AM-",
    "outputId": "e94f95ba-5f33-45a2-f306-fa73f4c0e8cb"
   },
   "outputs": [],
   "source": [
    "anns = []\n",
    "for (dirpath, dirnames, filenames) in os.walk(os.path.join(XVIEW_RECOGNITION_PATH, 'xview_recognition/xview_test')):\n",
    "    for filename in filenames:\n",
    "        image = GenericImage(os.path.join(dirpath,filename))\n",
    "        image.tile = np.array([0, 0, 224, 224])\n",
    "        obj = GenericObject()\n",
    "        obj.bb = (0, 0, 224, 224)\n",
    "        obj.category = dirpath[dirpath.rfind('/')+1:]\n",
    "        image.add_object(obj)\n",
    "        anns.append(image)\n",
    "print('Number of testing images: ' + str(len(anns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T13:01:22.837502Z",
     "iopub.status.busy": "2025-09-18T13:01:22.837113Z",
     "iopub.status.idle": "2025-09-18T13:03:24.835373Z",
     "shell.execute_reply": "2025-09-18T13:03:24.834773Z"
    },
    "executionInfo": {
     "elapsed": 208096,
     "status": "ok",
     "timestamp": 1759688825442,
     "user": {
      "displayName": "Jimmy “Jimmy”",
      "userId": "16087473281499178535"
     },
     "user_tz": -120
    },
    "id": "TGs2zqfv-AM_"
   },
   "outputs": [],
   "source": [
    "model.load_weights('model.keras')\n",
    "predictions_data = {\"images\": {}, \"annotations\": {}}\n",
    "for idx, ann in enumerate(anns):\n",
    "    image_data = {\"image_id\": ann.filename.split('/')[-1], \"filename\": ann.filename, \"width\": int(ann.tile[2]), \"height\": int(ann.tile[3])}\n",
    "    predictions_data[\"images\"][idx] = image_data\n",
    "    # Load image\n",
    "    image = load_geoimage(ann.filename)\n",
    "    for obj_pred in ann.objects:\n",
    "        # Generate prediction\n",
    "        warped_image = np.expand_dims(image, 0)\n",
    "        predictions = model.predict(warped_image, verbose=0)\n",
    "        # Save prediction\n",
    "        pred_category = list(categories.values())[np.argmax(predictions)]\n",
    "        pred_score = np.max(predictions)\n",
    "        annotation_data = {\"image_id\": ann.filename.split('/')[-1], \"category_id\": pred_category, \"bbox\": [int(x) for x in obj_pred.bb]}\n",
    "        predictions_data[\"annotations\"][idx] = annotation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T13:03:24.837935Z",
     "iopub.status.busy": "2025-09-18T13:03:24.837770Z",
     "iopub.status.idle": "2025-09-18T13:03:24.871502Z",
     "shell.execute_reply": "2025-09-18T13:03:24.870824Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1759688825444,
     "user": {
      "displayName": "Jimmy “Jimmy”",
      "userId": "16087473281499178535"
     },
     "user_tz": -120
    },
    "id": "fbJbKTB0zB20"
   },
   "outputs": [],
   "source": [
    "with open(\"prediction.json\", \"w\") as outfile:\n",
    "    json.dump(predictions_data, outfile)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
