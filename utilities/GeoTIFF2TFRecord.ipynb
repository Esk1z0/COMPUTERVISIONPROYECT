{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c031e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 19:53:44.882357: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-07 19:53:44.893796: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-10-07 19:53:44.908449: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8473] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-10-07 19:53:44.915414: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1471] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-07 19:53:44.931325: I tensorflow/core/platform/cpu_feature_guard.cc:211] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7dddb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución de categorías en el dataset:\n",
      "{'Cargo plane': 635, 'Small car': 3324, 'Bus': 1768, 'Truck': 2210, 'Motorboat': 1069, 'Fishing vessel': 706, 'Dump truck': 1236, 'Excavator': 789, 'Building': 3594, 'Helipad': 111, 'Storage tank': 1469, 'Shipping container': 1523, 'Pylon': 312}\n",
      "Number of training images: 16871\n",
      "Number of validation images: 1875\n"
     ]
    }
   ],
   "source": [
    "XVIEW_RECOGNITION_PATH = \"../data/xview_recognition\"\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "import numpy as np\n",
    "\n",
    "class GenericObject:\n",
    "    \"\"\"\n",
    "    Generic object data.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.id = uuid.uuid4()\n",
    "        self.bb = (-1, -1, -1, -1)\n",
    "        self.category= -1\n",
    "        self.score = -1\n",
    "\n",
    "class GenericImage:\n",
    "    \"\"\"\n",
    "    Generic image data.\n",
    "    \"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.tile = np.array([-1, -1, -1, -1])  # (pt_x, pt_y, pt_x+width, pt_y+height)\n",
    "        self.objects = list([])\n",
    "\n",
    "    def add_object(self, obj: GenericObject):\n",
    "        self.objects.append(obj)\n",
    "\n",
    "json_file = os.path.join(XVIEW_RECOGNITION_PATH, 'xview_ann_train.json')\n",
    "with open(json_file) as ifs:\n",
    "    json_data = json.load(ifs)\n",
    "ifs.close()\n",
    "\n",
    "categories = {0: 'Cargo plane', 1: 'Small car', 2: 'Bus', 3: 'Truck', 4: 'Motorboat', 5: 'Fishing vessel', 6: 'Dump truck', 7: 'Excavator', 8: 'Building', 9: 'Helipad', 10: 'Storage tank', 11: 'Shipping container', 12: 'Pylon'}\n",
    "\n",
    "id_to_name = {v: k for k, v in categories.items()}\n",
    "counts = {name: 0 for name in categories.values()}\n",
    "\n",
    "anns = []\n",
    "for json_img, json_ann in zip(json_data['images'].values(), json_data['annotations'].values()):\n",
    "    image = GenericImage(json_img['filename'])\n",
    "    image.tile = np.array([0, 0, json_img['width'], json_img['height']])\n",
    "    obj = GenericObject()\n",
    "    obj.bb = (int(json_ann['bbox'][0]), int(json_ann['bbox'][1]), int(json_ann['bbox'][2]), int(json_ann['bbox'][3]))\n",
    "    \n",
    "    category_name = json_ann['category_id']\n",
    "    if category_name != \"Unknown\":\n",
    "        counts[category_name] += 1\n",
    "    obj.category = id_to_name[category_name]\n",
    "\n",
    "    image.add_object(obj)\n",
    "    anns.append(image)\n",
    "\n",
    "print(\"Distribución de categorías en el dataset:\")\n",
    "print(counts)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "anns_train, anns_valid = train_test_split(anns, test_size=0.1, random_state=1, shuffle=True)\n",
    "print('Number of training images: ' + str(len(anns_train)))\n",
    "print('Number of validation images: ' + str(len(anns_valid)))\n",
    "\n",
    "objs_train = [(ann.filename, obj) for ann in anns_train for obj in ann.objects]\n",
    "objs_valid = [(ann.filename, obj) for ann in anns_valid for obj in ann.objects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6b94286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "def load_geoimage(filename_tensor):\n",
    "    warnings.filterwarnings('ignore', category=rasterio.errors.NotGeoreferencedWarning)\n",
    "    filename = filename_tensor\n",
    "    src_raster = rasterio.open(filename, 'r')\n",
    "\n",
    "    input_type = src_raster.profile['dtype']\n",
    "    input_channels = src_raster.count\n",
    "    img = np.zeros((src_raster.height, src_raster.width, src_raster.count), dtype=input_type)\n",
    "    for band in range(input_channels):\n",
    "        img[:, :, band] = src_raster.read(band+1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52ca90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Devuelve un bytes_list a partir de un string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy()\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Devuelve un int64_list a partir de un bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def serialize_example(image, category_id):\n",
    "    \"\"\"\n",
    "    Crea un tf.train.Example a partir de los datos de una imagen.\n",
    "    \"\"\"\n",
    "    image_bytes = image.tobytes()\n",
    "    \n",
    "    feature = {\n",
    "        'height': _int64_feature(image.shape[0]),\n",
    "        'width': _int64_feature(image.shape[1]),\n",
    "        'depth': _int64_feature(image.shape[2]),\n",
    "        'category_id': _int64_feature(category_id),\n",
    "        'image_raw': _bytes_feature(image_bytes),\n",
    "    }\n",
    "    \n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a35bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tfrecord(objs, output_filename):\n",
    "    print(f\"Escribiendo {output_filename}...\")\n",
    "    with tf.io.TFRecordWriter(output_filename) as writer:\n",
    "        for filename, obj in tqdm(objs):\n",
    "            image_path = os.path.abspath(os.path.join(XVIEW_RECOGNITION_PATH, filename))\n",
    "            image_np = load_geoimage(image_path)\n",
    "            category_id = obj.category\n",
    "            \n",
    "            serialized_example = serialize_example(image_np, category_id)\n",
    "            writer.write(serialized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db0c4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escribiendo train.tfrecord...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16871 [00:00<?, ?it/s]WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1759869269.733458    8208 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1759869269.871026    8208 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1759869269.871065    8208 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1759869269.876260    8208 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1759869269.876318    8208 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1759869269.876329    8208 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1759869270.072840    8208 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1759869270.073030    8208 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-10-07 20:34:30.073323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1759869270.073413    8208 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-10-07 20:34:30.073573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9732 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 5070 Ti, pci bus id: 0000:01:00.0, compute capability: 12.0\n",
      "100%|██████████| 16871/16871 [15:48<00:00, 17.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escribiendo validation.tfrecord...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [01:38<00:00, 18.96it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Conversión completada!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Ejecutar la conversión ---\n",
    "\n",
    "convert_to_tfrecord(objs_train, 'train.tfrecord')\n",
    "convert_to_tfrecord(objs_valid, 'validation.tfrecord')\n",
    "\n",
    "print(\"¡Conversión completada!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
