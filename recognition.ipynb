{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f47c1d0",
   "metadata": {},
   "source": [
    "## Assignment: Image recognition\n",
    "- Alumno 1: Yeray Martínez Martínez\n",
    "- Alumno 2: Juan Esteban Rincón Marín\n",
    "- Alumno 3: Kevin Oscar Arce Vera\n",
    "\n",
    "The goals of the assignment are:\n",
    "* Develop proficiency in using Tensorflow/Keras for training Neural Nets (NNs).\n",
    "* Put into practice the acquired knowledge to optimize the parameters and architecture of a feedforward Neural Net (ffNN), in the context of an image recognition problem.\n",
    "* Put into practice NNs specially conceived for analysing images. Design and optimize the parameters of a Convolutional Neural Net (CNN) to deal with previous task.\n",
    "* Train popular architectures from scratch (e.g., GoogLeNet, VGG, ResNet, ...), and compare the results with the ones provided by their pre-trained versions using transfer learning.\n",
    "\n",
    "Follow the link below to download the classification data set  “xview_recognition”: [https://drive.upm.es/s/2DDPE2zHw5dbM3G](https://drive.upm.es/s/2DDPE2zHw5dbM3G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0489a74",
   "metadata": {},
   "source": [
    "Lista de tareas para la entrega del 12 de octubre:\n",
    "\n",
    "- [ ] EDA de los datos de rigor (desbalanceo, tipos, forma, normalizacion, etc, etc, etc)\n",
    "- [ ] Data Augmentation para mejores datos y así a futuro está hecho\n",
    "- [ ] La red Feedforward\n",
    "- [ ] Análisis del entrenamiento y validación\n",
    "- [ ] Mejoras(Ya lo vamos viendo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e97dcc",
   "metadata": {},
   "source": [
    "# Ideas despues del seminario\n",
    "\n",
    "1. Usar SGDR porque converge más rápido, y aquí se cobra por tiempo, no sé vosotros.\n",
    "2. Lo de SB y LB está interesante, el pasar de batches pequeños a grandes ya sea con warm starting o dinamyc sampling\n",
    "3. Pensar a ver si hacemos al revés el batch, el ruido al principio parece que ayuda, ya sea ocn el lr o el Batch size.\n",
    "4. Aplicar SAM? ya pa tocar los huevos.\n",
    "5. no sé, que alguien haga un dibujito o algo pa poner en la portada del report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "528d6854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "78d0e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import numpy as np\n",
    "\n",
    "class GenericObject:\n",
    "    \"\"\"\n",
    "    Generic object data.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.id = uuid.uuid4()\n",
    "        self.bb = (-1, -1, -1, -1)\n",
    "        self.category= -1\n",
    "        self.score = -1\n",
    "\n",
    "class GenericImage:\n",
    "    \"\"\"\n",
    "    Generic image data.\n",
    "    \"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.tile = np.array([-1, -1, -1, -1])  # (pt_x, pt_y, pt_x+width, pt_y+height)\n",
    "        self.objects = list([])\n",
    "\n",
    "    def add_object(self, obj: GenericObject):\n",
    "        self.objects.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b1125599",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {0: 'Cargo plane', 1: 'Small car', 2: 'Bus', 3: 'Truck', 4: 'Motorboat', 5: 'Fishing vessel', 6: 'Dump truck', 7: 'Excavator', 8: 'Building', 9: 'Helipad', 10: 'Storage tank', 11: 'Shipping container', 12: 'Pylon'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "beb58360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "def load_geoimage(filename):\n",
    "    warnings.filterwarnings('ignore', category=rasterio.errors.NotGeoreferencedWarning)\n",
    "    src_raster = rasterio.open('./data/'+filename, 'r')\n",
    "    # RasterIO to OpenCV (see inconsistencies between libjpeg and libjpeg-turbo)\n",
    "    input_type = src_raster.profile['dtype']\n",
    "    input_channels = src_raster.count\n",
    "    img = np.zeros((src_raster.height, src_raster.width, src_raster.count), dtype=input_type)\n",
    "    for band in range(input_channels):\n",
    "        img[:, :, band] = src_raster.read(band+1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fe324d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load database\n",
    "json_file = './data/xview_ann_train.json'\n",
    "with open(json_file) as ifs:\n",
    "    json_data = json.load(ifs)\n",
    "ifs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a0d2cfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cargo plane': 635, 'Small car': 3324, 'Bus': 1768, 'Truck': 2210, 'Motorboat': 1069, 'Fishing vessel': 706, 'Dump truck': 1236, 'Excavator': 789, 'Building': 3594, 'Helipad': 111, 'Storage tank': 1469, 'Shipping container': 1523, 'Pylon': 312}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "counts = dict.fromkeys(categories.values(), 0)\n",
    "anns = []\n",
    "for json_img, json_ann in zip(json_data['images'].values(), json_data['annotations'].values()):\n",
    "    image = GenericImage(json_img['filename'])\n",
    "    image.tile = np.array([0, 0, json_img['width'], json_img['height']])\n",
    "    obj = GenericObject()\n",
    "    obj.bb = (int(json_ann['bbox'][0]), int(json_ann['bbox'][1]), int(json_ann['bbox'][2]), int(json_ann['bbox'][3]))\n",
    "    obj.category = json_ann['category_id']\n",
    "    # Resampling strategy to reduce training time\n",
    "    counts[obj.category] += 1\n",
    "    image.add_object(obj)\n",
    "    anns.append(image)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e17ee2",
   "metadata": {},
   "source": [
    "Tomando como punto de partida este código que tan amablemente nuestro profesor nos ha dejado vemos a simple vista que el número de imágenes por clase no es del todo equitativa. Para medir el desbalanceo de las imágenes primero haremos uso de una métrica.\n",
    "\n",
    "### Entropía de Clases\n",
    "Esta es una métrica que nos permite medir cuan desbalanceado está todo el dataset y nos permitirá comparar el estado actual con el estado del dataset después de rebalancear las clases.\n",
    "\n",
    "Este rango toma valores en el intervalo [0, log(k)] siendo k el número de clases, en este caso 13.\n",
    "Cuanto mas bajo mas balanceado está el dataset, cuanto más cerca de log(13) más desbalanceado.\n",
    "En caso de normalizar el valor el intervalo pasa a [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9bb7962e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropia de clases:  2.3142789779240545\n",
      "Valor normalizado:  0.9022708269821108\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "total = sum(counts.values())  \n",
    "entropy = -sum((i/total) * math.log(i/total) for i in counts.values())\n",
    "\n",
    "print(\"Entropia de clases: \", entropy)\n",
    "print(\"Valor normalizado: \", entropy/math.log(len(counts.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2a1c28",
   "metadata": {},
   "source": [
    "#### Observaciones\n",
    "\n",
    "Observamos que si bien es verdad que el dataset no tiene las clases perfectamente balanceadas, nos da un valor de 0.90227, lo que nos indica que no hay una clase que domine por encima de las demás."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "450ce793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_id': '0234476c-a017-4c9b-bd2f-872193076cca.tif',\n",
       " 'filename': 'xview_train/Building/0234476c-a017-4c9b-bd2f-872193076cca.tif',\n",
       " 'width': 224,\n",
       " 'height': 224}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data[\"images\"][\"0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594a30f9",
   "metadata": {},
   "source": [
    "## Imágenes\n",
    "Son archivos tiff que corresponden a una categoría de las 13.\n",
    "Los labels estan en onehot.\n",
    "La forma es de [224, 224, 3], lo que indica 3 canales de color e imágenes de 224x224.\n",
    "\n",
    "Para el modelo se usan lo que se conoce como \"generadores\" que permiten cargar las imágenes en memoria en cada batch en lugar de tenerlo en un dataset ocupando memoria a lo tonto. Esto desgraciadamente choca con el data augmentation, pero yo dejo lo de normalizacion y ya me contais que os parece si lo dejamos así o probamos a hacerlo con data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0898f22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 16871\n",
      "Number of validation images: 1875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "anns_train, anns_valid = train_test_split(anns, test_size=0.1, random_state=1, shuffle=True)\n",
    "print('Number of training images: ' + str(len(anns_train)))\n",
    "print('Number of validation images: ' + str(len(anns_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a46c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_images(objs, batch_size, do_shuffle=False):\n",
    "    while True:\n",
    "        if do_shuffle:\n",
    "            np.random.shuffle(objs)\n",
    "        groups = [objs[i:i+batch_size] for i in range(0, len(objs), batch_size)]\n",
    "        for group in groups:\n",
    "            images, labels = [], []\n",
    "            for (filename, obj) in group:\n",
    "                # Load image\n",
    "                image = load_geoimage(filename).astype(np.float32)\n",
    "                image = image/255 # minmax\n",
    "                images.append(image)\n",
    "                probabilities = np.zeros(len(categories))\n",
    "                probabilities[list(categories.values()).index(obj.category)] = 1\n",
    "                labels.append(probabilities)\n",
    "            images = np.array(images).astype(np.float32)\n",
    "            labels = np.array(labels).astype(np.float32)\n",
    "            yield images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "32864d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the list of objects from annotations\n",
    "objs_train = [(ann.filename, obj) for ann in anns_train for obj in ann.objects]\n",
    "objs_valid = [(ann.filename, obj) for ann in anns_valid for obj in ann.objects]\n",
    "# Generators\n",
    "batch_size = 16\n",
    "train_generator = generator_images(objs_train, batch_size, do_shuffle=True)\n",
    "valid_generator = generator_images(objs_valid, batch_size, do_shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2a5fe68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mínimo global: 0\n",
      "Máximo global: 255\n"
     ]
    }
   ],
   "source": [
    "#esta mierda es pa sacar el maximo de los valores, se puede borrar en realidad\n",
    "import numpy as np\n",
    "\n",
    "def global_minmax(objs):\n",
    "    gmin, gmax = float(\"inf\"), float(\"-inf\")\n",
    "    for (filename, obj) in objs:\n",
    "        img = load_geoimage(filename)   # (H, W, C)\n",
    "        local_min = np.min(img)\n",
    "        local_max = np.max(img)\n",
    "        if local_min < gmin:\n",
    "            gmin = local_min\n",
    "        if local_max > gmax:\n",
    "            gmax = local_max\n",
    "    return gmin, gmax\n",
    "\n",
    "# Uso\n",
    "mn, mx = global_minmax(objs_train)\n",
    "print(\"Mínimo global:\", mn)\n",
    "print(\"Máximo global:\", mx)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
